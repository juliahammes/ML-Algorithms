{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='green'>Random Forest</font>"
      ],
      "metadata": {
        "id": "dUM69e37LR8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entendermos como o algoritmo Random Forest funciona é essencial o entendimento sobre **árvores de decisão**, cujo resumo pode ser encontrado [aqui](https://github.com/juliahammes/ML-Algorithms/blob/main/algorithms_summary/decision_trees/Decision_Trees.ipynb).\n",
        "\n",
        "Esse conhecimento será necessário pois Random Forest, ou em português, Floresta Aleatória é um algoritmo de machine learning composto por muitas árvores de decisão, geradas de forma aleatória, que irão operar como um ensemble* e irão gerar o resultado final de acordo com a classe mais votada.\n",
        "\n",
        "*Em resumo, **ensemble** é uma técnica que combina diversos classificadores para fornecer soluções para probemas complexos.*\n",
        "\n"
      ],
      "metadata": {
        "id": "mhiltdQ_ys4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Como o algoritmo funciona"
      ],
      "metadata": {
        "id": "tgOTMVYLzs0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passo 1** - Gerar um novo dataset\n",
        "\n",
        "Para criar uma árvore de decisão, o primeiro passo do algoritmo Random Forest será criar um dataset, do mesmo tamanho do dataset original, selecionando, de maneira aleatória, com o método bootstrap*, algumas amostras dos dados de treinamento.\n",
        "\n",
        "Em resumo, **bootstrap** é uma tecnica para realizar amostragens de tamanho igual ao da amostra original com reposição da mesma, ou seja, onde uma amostra já selecionada poderá ser selecionada novamente."
      ],
      "metadata": {
        "id": "heB2LA1S8id6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passo 2** - Criar a árvore de decisão\n",
        "\n",
        "Aqui o primeiro passo, conforme vimos no [resumo](https://github.com/juliahammes/ML-Algorithms/blob/main/algorithms_summary/decision_trees/Decision_Trees.ipynb) sobre árvores de decisão, será selecionar o nó raiz, porém diferente do algoritmo de árvores de decisão, o Random Forest não considera todas as features mas sim um subconjunto de 2 ou mais features, selecionadas de forma aleatória, para tomar essa decisão.\n",
        "Para escolher a feature do próximo nó, assim como anteriormente, o algoritmo irá selecionar um novo subconjunto de 2 ou mais features, excluindo aquela já selecionada e o processo se repetirá até o último nó."
      ],
      "metadata": {
        "id": "DJQEd6kt8dnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passo 3** - Criar a Floresta Aleatória\n",
        "\n",
        "Repetimos então os Passos 1 e 2, ou seja, criando um novo dataset com boostrap e gerando uma nova árvore usando apenas um subconjunto de features aleatórias a cada passo. Gerando assim, muitas árvores de decisão diferentes que irão compor a Floresta Aleatória."
      ],
      "metadata": {
        "id": "XfUyjKyf_YiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Predição\n",
        "\n",
        "Para realizar a predição, verifica-se o resultado de cada árvore, em problemas de classificação a classe que mais vezes foi apresentada será a predição final e em problemas de regressão será realizada uma média dos valores previstos por cada árvore de decisão."
      ],
      "metadata": {
        "id": "Esl0JZRXBcUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prós\n",
        "\n",
        "1.   Pode ser usado com grandes datasets de forma eficiente;\n",
        "2.   Tem um nível de acurácia muito maior do que usando árvore de decisão e reduz o overfitting;\n",
        "3.   Pode ser utilizado tanto para regressão como para classificação.\n",
        "4.   Não requer que os dados estejam normalizados."
      ],
      "metadata": {
        "id": "d83MJpXACaTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contras\n",
        "\n",
        "1.   Requer mais recusos computacionais;\n",
        "2.   Consome mais tempo comparado com o algoritmo de árvore de decisão."
      ],
      "metadata": {
        "id": "5wortv57DW5B"
      }
    }
  ]
}